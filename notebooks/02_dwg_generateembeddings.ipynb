{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c297c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-06 21:13:05.726\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mgelos.config\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m16\u001b[0m - \u001b[1mPROJ_ROOT path is: /app\u001b[0m\n",
      "/opt/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:albumentations.check_version:A new version of Albumentations is available: 2.0.8 (you have 1.4.10). Upgrade using: pip install --upgrade albumentations\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "from gelos.gelosdatamodule import GELOSDataModule\n",
    "import yaml\n",
    "from gelos import config\n",
    "from lightning.pytorch import Trainer\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from gelos.config import PROJ_ROOT, EXTERNAL_DATA_DIR, PROCESSED_DATA_DIR, DATA_VERSION, RAW_DATA_DIR\n",
    "from terratorch.tasks import EmbeddingGenerationTask\n",
    "from gelos.features import LenientEmbeddingGenerationTask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6c5338b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.cli import instantiate_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1a7a3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_file = \"prithvi_eo_300m_embedding_generation.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ebce611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/app/gelos/configs/prithvi_eo_300m_embedding_generation.yaml\n"
     ]
    }
   ],
   "source": [
    "yaml_config_directory = PROJ_ROOT / 'gelos' / 'configs'\n",
    "yaml_path = yaml_config_directory / yaml_file\n",
    "print(yaml_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a57863",
   "metadata": {},
   "source": [
    "## Run Embedding Generation step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "970fcac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.11/site-packages/terratorch/tasks/embedding_generation.py:67: UserWarning: GeoTIFF selected; 2D token embeddings (ViT) will be reshaped to [C, sqrt(num_tokens), sqrt(num_tokens)] after dropping CLS if present.\n",
      "  warnings.warn(\n",
      "INFO:terratorch.models.backbones.prithvi_vit:model_bands not passed. Assuming bands are ordered in the same way as [<HLSBands.BLUE: 'BLUE'>, <HLSBands.GREEN: 'GREEN'>, <HLSBands.RED: 'RED'>, <HLSBands.NIR_NARROW: 'NIR_NARROW'>, <HLSBands.SWIR_1: 'SWIR_1'>, <HLSBands.SWIR_2: 'SWIR_2'>].Pretrained patch_embed layer may be misaligned with current bands\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:\n",
      "  class_path: gelos.gelosdatamodule.GELOSDataModule\n",
      "  init_args:\n",
      "    bands:\n",
      "      S2L2A:\n",
      "      - BLUE\n",
      "      - GREEN\n",
      "      - RED\n",
      "      - NIR_NARROW\n",
      "      - SWIR_1\n",
      "      - SWIR_2\n",
      "    batch_size: 1\n",
      "    num_workers: 0\n",
      "embedding_extraction_strategies:\n",
      "  All Patches from April to June:\n",
      "  - start: 37\n",
      "    step: 1\n",
      "    stop: 73\n",
      "  All Steps of Middle Patch:\n",
      "  - start: 19\n",
      "    step: 36\n",
      "    stop: null\n",
      "  CLS Token:\n",
      "  - start: 0\n",
      "    step: 1\n",
      "    stop: 1\n",
      "model:\n",
      "  class_path: terratorch.tasks.EmbeddingGenerationTask\n",
      "  init_args:\n",
      "    embed_file_key: filename\n",
      "    embedding_pooling: null\n",
      "    has_cls: true\n",
      "    model: prithvi_eo_v2_300\n",
      "    model_args:\n",
      "      backbone: prithvi_eo_v2_300\n",
      "      backbone_bands:\n",
      "      - BLUE\n",
      "      - GREEN\n",
      "      - RED\n",
      "      - NIR_NARROW\n",
      "      - SWIR_1\n",
      "      - SWIR_2\n",
      "      backbone_pretrained: true\n",
      "    output_format: parquet\n",
      "  title: Prithvi EO V2 300M\n",
      "seed_everything: 0\n",
      "trainer:\n",
      "  accelerator: auto\n",
      "  callbacks: []\n",
      "  devices: auto\n",
      "  max_epochs: 0\n",
      "  num_nodes: 1\n",
      "  strategy: auto\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "with open(yaml_path, \"r\") as f:\n",
    "        yaml_config = yaml.safe_load(f)\n",
    "\n",
    "print(yaml.dump(yaml_config))\n",
    "\n",
    "model_name = yaml_config['model']['init_args']['model']\n",
    "output_dir = PROCESSED_DATA_DIR / DATA_VERSION / model_name\n",
    "output_dir.mkdir(exist_ok=True, parents=True)\n",
    "data_root = RAW_DATA_DIR / DATA_VERSION\n",
    "\n",
    "# add variables to yaml config so it can be passed to classes\n",
    "yaml_config['data']['init_args']['data_root'] = data_root\n",
    "yaml_config['model']['init_args']['output_dir'] = output_dir\n",
    "\n",
    "# instantiate transform classes if they exist\n",
    "if \"transform\" in yaml_config[\"data\"][\"init_args\"].keys():\n",
    "      yaml_config[\"data\"][\"init_args\"][\"transform\"] = [\n",
    "            instantiate_class(args = (), init=class_path) for class_path in yaml_config[\"data\"][\"init_args\"][\"transform\"]\n",
    "      ]\n",
    "gelos_datamodule = GELOSDataModule(**yaml_config['data']['init_args'])\n",
    "task = LenientEmbeddingGenerationTask(**yaml_config['model']['init_args'])\n",
    "\n",
    "device = 'gpu' if torch.cuda.is_available() else 'cpu'\n",
    "trainer = Trainer(accelerator=device, devices=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258dc376",
   "metadata": {},
   "source": [
    "### Inspect model and ensure weights have been loaded correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8086141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cls_token torch.Size([1, 1, 1024])\n",
      "patch_embed.proj.weight torch.Size([1024, 6, 1, 16, 16])\n",
      "patch_embed.proj.bias torch.Size([1024])\n",
      "blocks.0.norm1.weight torch.Size([1024])\n",
      "blocks.0.norm1.bias torch.Size([1024])\n",
      "blocks.0.attn.qkv.weight torch.Size([3072, 1024])\n",
      "blocks.0.attn.qkv.bias torch.Size([3072])\n",
      "blocks.0.attn.proj.weight torch.Size([1024, 1024])\n",
      "blocks.0.attn.proj.bias torch.Size([1024])\n",
      "blocks.0.norm2.weight torch.Size([1024])\n",
      "blocks.0.norm2.bias torch.Size([1024])\n",
      "blocks.0.mlp.fc1.weight torch.Size([4096, 1024])\n",
      "blocks.0.mlp.fc1.bias torch.Size([4096])\n",
      "blocks.0.mlp.fc2.weight torch.Size([1024, 4096])\n",
      "blocks.0.mlp.fc2.bias torch.Size([1024])\n",
      "blocks.1.norm1.weight torch.Size([1024])\n",
      "blocks.1.norm1.bias torch.Size([1024])\n",
      "blocks.1.attn.qkv.weight torch.Size([3072, 1024])\n",
      "blocks.1.attn.qkv.bias torch.Size([3072])\n",
      "blocks.1.attn.proj.weight torch.Size([1024, 1024])\n",
      "blocks.1.attn.proj.bias torch.Size([1024])\n",
      "blocks.1.norm2.weight torch.Size([1024])\n",
      "blocks.1.norm2.bias torch.Size([1024])\n",
      "blocks.1.mlp.fc1.weight torch.Size([4096, 1024])\n",
      "blocks.1.mlp.fc1.bias torch.Size([4096])\n",
      "blocks.1.mlp.fc2.weight torch.Size([1024, 4096])\n",
      "blocks.1.mlp.fc2.bias torch.Size([1024])\n",
      "blocks.2.norm1.weight torch.Size([1024])\n",
      "blocks.2.norm1.bias torch.Size([1024])\n",
      "blocks.2.attn.qkv.weight torch.Size([3072, 1024])\n",
      "blocks.2.attn.qkv.bias torch.Size([3072])\n",
      "blocks.2.attn.proj.weight torch.Size([1024, 1024])\n",
      "blocks.2.attn.proj.bias torch.Size([1024])\n",
      "blocks.2.norm2.weight torch.Size([1024])\n",
      "blocks.2.norm2.bias torch.Size([1024])\n",
      "blocks.2.mlp.fc1.weight torch.Size([4096, 1024])\n",
      "blocks.2.mlp.fc1.bias torch.Size([4096])\n",
      "blocks.2.mlp.fc2.weight torch.Size([1024, 4096])\n",
      "blocks.2.mlp.fc2.bias torch.Size([1024])\n",
      "blocks.3.norm1.weight torch.Size([1024])\n",
      "blocks.3.norm1.bias torch.Size([1024])\n",
      "blocks.3.attn.qkv.weight torch.Size([3072, 1024])\n",
      "blocks.3.attn.qkv.bias torch.Size([3072])\n",
      "blocks.3.attn.proj.weight torch.Size([1024, 1024])\n",
      "blocks.3.attn.proj.bias torch.Size([1024])\n",
      "blocks.3.norm2.weight torch.Size([1024])\n",
      "blocks.3.norm2.bias torch.Size([1024])\n",
      "blocks.3.mlp.fc1.weight torch.Size([4096, 1024])\n",
      "blocks.3.mlp.fc1.bias torch.Size([4096])\n",
      "blocks.3.mlp.fc2.weight torch.Size([1024, 4096])\n",
      "blocks.3.mlp.fc2.bias torch.Size([1024])\n",
      "blocks.4.norm1.weight torch.Size([1024])\n",
      "blocks.4.norm1.bias torch.Size([1024])\n",
      "blocks.4.attn.qkv.weight torch.Size([3072, 1024])\n",
      "blocks.4.attn.qkv.bias torch.Size([3072])\n",
      "blocks.4.attn.proj.weight torch.Size([1024, 1024])\n",
      "blocks.4.attn.proj.bias torch.Size([1024])\n",
      "blocks.4.norm2.weight torch.Size([1024])\n",
      "blocks.4.norm2.bias torch.Size([1024])\n",
      "blocks.4.mlp.fc1.weight torch.Size([4096, 1024])\n",
      "blocks.4.mlp.fc1.bias torch.Size([4096])\n",
      "blocks.4.mlp.fc2.weight torch.Size([1024, 4096])\n",
      "blocks.4.mlp.fc2.bias torch.Size([1024])\n",
      "blocks.5.norm1.weight torch.Size([1024])\n",
      "blocks.5.norm1.bias torch.Size([1024])\n",
      "blocks.5.attn.qkv.weight torch.Size([3072, 1024])\n",
      "blocks.5.attn.qkv.bias torch.Size([3072])\n",
      "blocks.5.attn.proj.weight torch.Size([1024, 1024])\n",
      "blocks.5.attn.proj.bias torch.Size([1024])\n",
      "blocks.5.norm2.weight torch.Size([1024])\n",
      "blocks.5.norm2.bias torch.Size([1024])\n",
      "blocks.5.mlp.fc1.weight torch.Size([4096, 1024])\n",
      "blocks.5.mlp.fc1.bias torch.Size([4096])\n",
      "blocks.5.mlp.fc2.weight torch.Size([1024, 4096])\n",
      "blocks.5.mlp.fc2.bias torch.Size([1024])\n",
      "blocks.6.norm1.weight torch.Size([1024])\n",
      "blocks.6.norm1.bias torch.Size([1024])\n",
      "blocks.6.attn.qkv.weight torch.Size([3072, 1024])\n",
      "blocks.6.attn.qkv.bias torch.Size([3072])\n",
      "blocks.6.attn.proj.weight torch.Size([1024, 1024])\n",
      "blocks.6.attn.proj.bias torch.Size([1024])\n",
      "blocks.6.norm2.weight torch.Size([1024])\n",
      "blocks.6.norm2.bias torch.Size([1024])\n",
      "blocks.6.mlp.fc1.weight torch.Size([4096, 1024])\n",
      "blocks.6.mlp.fc1.bias torch.Size([4096])\n",
      "blocks.6.mlp.fc2.weight torch.Size([1024, 4096])\n",
      "blocks.6.mlp.fc2.bias torch.Size([1024])\n",
      "blocks.7.norm1.weight torch.Size([1024])\n",
      "blocks.7.norm1.bias torch.Size([1024])\n",
      "blocks.7.attn.qkv.weight torch.Size([3072, 1024])\n",
      "blocks.7.attn.qkv.bias torch.Size([3072])\n",
      "blocks.7.attn.proj.weight torch.Size([1024, 1024])\n",
      "blocks.7.attn.proj.bias torch.Size([1024])\n",
      "blocks.7.norm2.weight torch.Size([1024])\n",
      "blocks.7.norm2.bias torch.Size([1024])\n",
      "blocks.7.mlp.fc1.weight torch.Size([4096, 1024])\n",
      "blocks.7.mlp.fc1.bias torch.Size([4096])\n",
      "blocks.7.mlp.fc2.weight torch.Size([1024, 4096])\n",
      "blocks.7.mlp.fc2.bias torch.Size([1024])\n",
      "blocks.8.norm1.weight torch.Size([1024])\n",
      "blocks.8.norm1.bias torch.Size([1024])\n",
      "blocks.8.attn.qkv.weight torch.Size([3072, 1024])\n",
      "blocks.8.attn.qkv.bias torch.Size([3072])\n",
      "blocks.8.attn.proj.weight torch.Size([1024, 1024])\n",
      "blocks.8.attn.proj.bias torch.Size([1024])\n",
      "blocks.8.norm2.weight torch.Size([1024])\n",
      "blocks.8.norm2.bias torch.Size([1024])\n",
      "blocks.8.mlp.fc1.weight torch.Size([4096, 1024])\n",
      "blocks.8.mlp.fc1.bias torch.Size([4096])\n",
      "blocks.8.mlp.fc2.weight torch.Size([1024, 4096])\n",
      "blocks.8.mlp.fc2.bias torch.Size([1024])\n",
      "blocks.9.norm1.weight torch.Size([1024])\n",
      "blocks.9.norm1.bias torch.Size([1024])\n",
      "blocks.9.attn.qkv.weight torch.Size([3072, 1024])\n",
      "blocks.9.attn.qkv.bias torch.Size([3072])\n",
      "blocks.9.attn.proj.weight torch.Size([1024, 1024])\n",
      "blocks.9.attn.proj.bias torch.Size([1024])\n",
      "blocks.9.norm2.weight torch.Size([1024])\n",
      "blocks.9.norm2.bias torch.Size([1024])\n",
      "blocks.9.mlp.fc1.weight torch.Size([4096, 1024])\n",
      "blocks.9.mlp.fc1.bias torch.Size([4096])\n",
      "blocks.9.mlp.fc2.weight torch.Size([1024, 4096])\n",
      "blocks.9.mlp.fc2.bias torch.Size([1024])\n",
      "blocks.10.norm1.weight torch.Size([1024])\n",
      "blocks.10.norm1.bias torch.Size([1024])\n",
      "blocks.10.attn.qkv.weight torch.Size([3072, 1024])\n",
      "blocks.10.attn.qkv.bias torch.Size([3072])\n",
      "blocks.10.attn.proj.weight torch.Size([1024, 1024])\n",
      "blocks.10.attn.proj.bias torch.Size([1024])\n",
      "blocks.10.norm2.weight torch.Size([1024])\n",
      "blocks.10.norm2.bias torch.Size([1024])\n",
      "blocks.10.mlp.fc1.weight torch.Size([4096, 1024])\n",
      "blocks.10.mlp.fc1.bias torch.Size([4096])\n",
      "blocks.10.mlp.fc2.weight torch.Size([1024, 4096])\n",
      "blocks.10.mlp.fc2.bias torch.Size([1024])\n",
      "blocks.11.norm1.weight torch.Size([1024])\n",
      "blocks.11.norm1.bias torch.Size([1024])\n",
      "blocks.11.attn.qkv.weight torch.Size([3072, 1024])\n",
      "blocks.11.attn.qkv.bias torch.Size([3072])\n",
      "blocks.11.attn.proj.weight torch.Size([1024, 1024])\n",
      "blocks.11.attn.proj.bias torch.Size([1024])\n",
      "blocks.11.norm2.weight torch.Size([1024])\n",
      "blocks.11.norm2.bias torch.Size([1024])\n",
      "blocks.11.mlp.fc1.weight torch.Size([4096, 1024])\n",
      "blocks.11.mlp.fc1.bias torch.Size([4096])\n",
      "blocks.11.mlp.fc2.weight torch.Size([1024, 4096])\n",
      "blocks.11.mlp.fc2.bias torch.Size([1024])\n",
      "blocks.12.norm1.weight torch.Size([1024])\n",
      "blocks.12.norm1.bias torch.Size([1024])\n",
      "blocks.12.attn.qkv.weight torch.Size([3072, 1024])\n",
      "blocks.12.attn.qkv.bias torch.Size([3072])\n",
      "blocks.12.attn.proj.weight torch.Size([1024, 1024])\n",
      "blocks.12.attn.proj.bias torch.Size([1024])\n",
      "blocks.12.norm2.weight torch.Size([1024])\n",
      "blocks.12.norm2.bias torch.Size([1024])\n",
      "blocks.12.mlp.fc1.weight torch.Size([4096, 1024])\n",
      "blocks.12.mlp.fc1.bias torch.Size([4096])\n",
      "blocks.12.mlp.fc2.weight torch.Size([1024, 4096])\n",
      "blocks.12.mlp.fc2.bias torch.Size([1024])\n",
      "blocks.13.norm1.weight torch.Size([1024])\n",
      "blocks.13.norm1.bias torch.Size([1024])\n",
      "blocks.13.attn.qkv.weight torch.Size([3072, 1024])\n",
      "blocks.13.attn.qkv.bias torch.Size([3072])\n",
      "blocks.13.attn.proj.weight torch.Size([1024, 1024])\n",
      "blocks.13.attn.proj.bias torch.Size([1024])\n",
      "blocks.13.norm2.weight torch.Size([1024])\n",
      "blocks.13.norm2.bias torch.Size([1024])\n",
      "blocks.13.mlp.fc1.weight torch.Size([4096, 1024])\n",
      "blocks.13.mlp.fc1.bias torch.Size([4096])\n",
      "blocks.13.mlp.fc2.weight torch.Size([1024, 4096])\n",
      "blocks.13.mlp.fc2.bias torch.Size([1024])\n",
      "blocks.14.norm1.weight torch.Size([1024])\n",
      "blocks.14.norm1.bias torch.Size([1024])\n",
      "blocks.14.attn.qkv.weight torch.Size([3072, 1024])\n",
      "blocks.14.attn.qkv.bias torch.Size([3072])\n",
      "blocks.14.attn.proj.weight torch.Size([1024, 1024])\n",
      "blocks.14.attn.proj.bias torch.Size([1024])\n",
      "blocks.14.norm2.weight torch.Size([1024])\n",
      "blocks.14.norm2.bias torch.Size([1024])\n",
      "blocks.14.mlp.fc1.weight torch.Size([4096, 1024])\n",
      "blocks.14.mlp.fc1.bias torch.Size([4096])\n",
      "blocks.14.mlp.fc2.weight torch.Size([1024, 4096])\n",
      "blocks.14.mlp.fc2.bias torch.Size([1024])\n",
      "blocks.15.norm1.weight torch.Size([1024])\n",
      "blocks.15.norm1.bias torch.Size([1024])\n",
      "blocks.15.attn.qkv.weight torch.Size([3072, 1024])\n",
      "blocks.15.attn.qkv.bias torch.Size([3072])\n",
      "blocks.15.attn.proj.weight torch.Size([1024, 1024])\n",
      "blocks.15.attn.proj.bias torch.Size([1024])\n",
      "blocks.15.norm2.weight torch.Size([1024])\n",
      "blocks.15.norm2.bias torch.Size([1024])\n",
      "blocks.15.mlp.fc1.weight torch.Size([4096, 1024])\n",
      "blocks.15.mlp.fc1.bias torch.Size([4096])\n",
      "blocks.15.mlp.fc2.weight torch.Size([1024, 4096])\n",
      "blocks.15.mlp.fc2.bias torch.Size([1024])\n",
      "blocks.16.norm1.weight torch.Size([1024])\n",
      "blocks.16.norm1.bias torch.Size([1024])\n",
      "blocks.16.attn.qkv.weight torch.Size([3072, 1024])\n",
      "blocks.16.attn.qkv.bias torch.Size([3072])\n",
      "blocks.16.attn.proj.weight torch.Size([1024, 1024])\n",
      "blocks.16.attn.proj.bias torch.Size([1024])\n",
      "blocks.16.norm2.weight torch.Size([1024])\n",
      "blocks.16.norm2.bias torch.Size([1024])\n",
      "blocks.16.mlp.fc1.weight torch.Size([4096, 1024])\n",
      "blocks.16.mlp.fc1.bias torch.Size([4096])\n",
      "blocks.16.mlp.fc2.weight torch.Size([1024, 4096])\n",
      "blocks.16.mlp.fc2.bias torch.Size([1024])\n",
      "blocks.17.norm1.weight torch.Size([1024])\n",
      "blocks.17.norm1.bias torch.Size([1024])\n",
      "blocks.17.attn.qkv.weight torch.Size([3072, 1024])\n",
      "blocks.17.attn.qkv.bias torch.Size([3072])\n",
      "blocks.17.attn.proj.weight torch.Size([1024, 1024])\n",
      "blocks.17.attn.proj.bias torch.Size([1024])\n",
      "blocks.17.norm2.weight torch.Size([1024])\n",
      "blocks.17.norm2.bias torch.Size([1024])\n",
      "blocks.17.mlp.fc1.weight torch.Size([4096, 1024])\n",
      "blocks.17.mlp.fc1.bias torch.Size([4096])\n",
      "blocks.17.mlp.fc2.weight torch.Size([1024, 4096])\n",
      "blocks.17.mlp.fc2.bias torch.Size([1024])\n",
      "blocks.18.norm1.weight torch.Size([1024])\n",
      "blocks.18.norm1.bias torch.Size([1024])\n",
      "blocks.18.attn.qkv.weight torch.Size([3072, 1024])\n",
      "blocks.18.attn.qkv.bias torch.Size([3072])\n",
      "blocks.18.attn.proj.weight torch.Size([1024, 1024])\n",
      "blocks.18.attn.proj.bias torch.Size([1024])\n",
      "blocks.18.norm2.weight torch.Size([1024])\n",
      "blocks.18.norm2.bias torch.Size([1024])\n",
      "blocks.18.mlp.fc1.weight torch.Size([4096, 1024])\n",
      "blocks.18.mlp.fc1.bias torch.Size([4096])\n",
      "blocks.18.mlp.fc2.weight torch.Size([1024, 4096])\n",
      "blocks.18.mlp.fc2.bias torch.Size([1024])\n",
      "blocks.19.norm1.weight torch.Size([1024])\n",
      "blocks.19.norm1.bias torch.Size([1024])\n",
      "blocks.19.attn.qkv.weight torch.Size([3072, 1024])\n",
      "blocks.19.attn.qkv.bias torch.Size([3072])\n",
      "blocks.19.attn.proj.weight torch.Size([1024, 1024])\n",
      "blocks.19.attn.proj.bias torch.Size([1024])\n",
      "blocks.19.norm2.weight torch.Size([1024])\n",
      "blocks.19.norm2.bias torch.Size([1024])\n",
      "blocks.19.mlp.fc1.weight torch.Size([4096, 1024])\n",
      "blocks.19.mlp.fc1.bias torch.Size([4096])\n",
      "blocks.19.mlp.fc2.weight torch.Size([1024, 4096])\n",
      "blocks.19.mlp.fc2.bias torch.Size([1024])\n",
      "blocks.20.norm1.weight torch.Size([1024])\n",
      "blocks.20.norm1.bias torch.Size([1024])\n",
      "blocks.20.attn.qkv.weight torch.Size([3072, 1024])\n",
      "blocks.20.attn.qkv.bias torch.Size([3072])\n",
      "blocks.20.attn.proj.weight torch.Size([1024, 1024])\n",
      "blocks.20.attn.proj.bias torch.Size([1024])\n",
      "blocks.20.norm2.weight torch.Size([1024])\n",
      "blocks.20.norm2.bias torch.Size([1024])\n",
      "blocks.20.mlp.fc1.weight torch.Size([4096, 1024])\n",
      "blocks.20.mlp.fc1.bias torch.Size([4096])\n",
      "blocks.20.mlp.fc2.weight torch.Size([1024, 4096])\n",
      "blocks.20.mlp.fc2.bias torch.Size([1024])\n",
      "blocks.21.norm1.weight torch.Size([1024])\n",
      "blocks.21.norm1.bias torch.Size([1024])\n",
      "blocks.21.attn.qkv.weight torch.Size([3072, 1024])\n",
      "blocks.21.attn.qkv.bias torch.Size([3072])\n",
      "blocks.21.attn.proj.weight torch.Size([1024, 1024])\n",
      "blocks.21.attn.proj.bias torch.Size([1024])\n",
      "blocks.21.norm2.weight torch.Size([1024])\n",
      "blocks.21.norm2.bias torch.Size([1024])\n",
      "blocks.21.mlp.fc1.weight torch.Size([4096, 1024])\n",
      "blocks.21.mlp.fc1.bias torch.Size([4096])\n",
      "blocks.21.mlp.fc2.weight torch.Size([1024, 4096])\n",
      "blocks.21.mlp.fc2.bias torch.Size([1024])\n",
      "blocks.22.norm1.weight torch.Size([1024])\n",
      "blocks.22.norm1.bias torch.Size([1024])\n",
      "blocks.22.attn.qkv.weight torch.Size([3072, 1024])\n",
      "blocks.22.attn.qkv.bias torch.Size([3072])\n",
      "blocks.22.attn.proj.weight torch.Size([1024, 1024])\n",
      "blocks.22.attn.proj.bias torch.Size([1024])\n",
      "blocks.22.norm2.weight torch.Size([1024])\n",
      "blocks.22.norm2.bias torch.Size([1024])\n",
      "blocks.22.mlp.fc1.weight torch.Size([4096, 1024])\n",
      "blocks.22.mlp.fc1.bias torch.Size([4096])\n",
      "blocks.22.mlp.fc2.weight torch.Size([1024, 4096])\n",
      "blocks.22.mlp.fc2.bias torch.Size([1024])\n",
      "blocks.23.norm1.weight torch.Size([1024])\n",
      "blocks.23.norm1.bias torch.Size([1024])\n",
      "blocks.23.attn.qkv.weight torch.Size([3072, 1024])\n",
      "blocks.23.attn.qkv.bias torch.Size([3072])\n",
      "blocks.23.attn.proj.weight torch.Size([1024, 1024])\n",
      "blocks.23.attn.proj.bias torch.Size([1024])\n",
      "blocks.23.norm2.weight torch.Size([1024])\n",
      "blocks.23.norm2.bias torch.Size([1024])\n",
      "blocks.23.mlp.fc1.weight torch.Size([4096, 1024])\n",
      "blocks.23.mlp.fc1.bias torch.Size([4096])\n",
      "blocks.23.mlp.fc2.weight torch.Size([1024, 4096])\n",
      "blocks.23.mlp.fc2.bias torch.Size([1024])\n",
      "norm.weight torch.Size([1024])\n",
      "norm.bias torch.Size([1024])\n"
     ]
    }
   ],
   "source": [
    "for k, v in task.model.named_parameters():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77bf87e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.prithvi_eo_v2 import PrithviViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b03acc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_version = \"300M\"\n",
    "prithvi_model = PrithviViT(num_frames = 4, in_chans = 6, model_size = model_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b864706",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_path = EXTERNAL_DATA_DIR / \"model_weights\"/ f\"Prithvi_EO_V2_{model_version}.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b734ec1c",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/app/data/external/model_weights/Prithvi_EO_V2_300M.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m weights = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/venv/lib/python3.11/site-packages/torch/serialization.py:1484\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1481\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args.keys():\n\u001b[32m   1482\u001b[39m     pickle_load_args[\u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1484\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[32m   1485\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[32m   1486\u001b[39m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[32m   1487\u001b[39m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[32m   1488\u001b[39m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[32m   1489\u001b[39m         orig_position = opened_file.tell()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/venv/lib/python3.11/site-packages/torch/serialization.py:759\u001b[39m, in \u001b[36m_open_file_like\u001b[39m\u001b[34m(name_or_buffer, mode)\u001b[39m\n\u001b[32m    757\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_open_file_like\u001b[39m(name_or_buffer: FileLike, mode: \u001b[38;5;28mstr\u001b[39m) -> _opener[IO[\u001b[38;5;28mbytes\u001b[39m]]:\n\u001b[32m    758\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[32m--> \u001b[39m\u001b[32m759\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    760\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    761\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/venv/lib/python3.11/site-packages/torch/serialization.py:740\u001b[39m, in \u001b[36m_open_file.__init__\u001b[39m\u001b[34m(self, name, mode)\u001b[39m\n\u001b[32m    739\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike[\u001b[38;5;28mstr\u001b[39m]], mode: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m740\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/app/data/external/model_weights/Prithvi_EO_V2_300M.pt'"
     ]
    }
   ],
   "source": [
    "weights = torch.load(weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f68474",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_state_dict = {}\n",
    "for k, v in state_dict.items():\n",
    "    if 'pos_embed' in k:\n",
    "        continue\n",
    "    if k.startswith('encoder'):\n",
    "        new_key = k.replace(\"encoder.\", \"\", 1)\n",
    "        encoder_state_dict[new_key] = v\n",
    "\n",
    "prithvi_model.load_state_dict(encoder_state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c684f47",
   "metadata": {},
   "source": [
    "### Inspect outputs of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf96cbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gelos_datamodule.setup(stage=\"predict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2d716e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S2L2A (4, 96, 96, 12)\n",
      "S1RTC (4, 96, 96, 2)\n",
      "DEM (4, 32, 32, 1)\n",
      "S2L2A torch.Size([12, 4, 96, 96])\n",
      "S1RTC torch.Size([2, 4, 96, 96])\n",
      "DEM torch.Size([1, 4, 96, 96])\n",
      "S2L2A torch.Size([12, 4, 96, 96])\n",
      "S1RTC torch.Size([2, 4, 96, 96])\n",
      "DEM torch.Size([1, 4, 96, 96])\n",
      "filename 000000\n",
      "file_id 0\n"
     ]
    }
   ],
   "source": [
    "for k, v in gelos_datamodule.dataset[0].items():\n",
    "    if k == \"image\":\n",
    "        for sensor, data in v.items():\n",
    "            print(sensor, data.shape)\n",
    "    else:\n",
    "        print(k, v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
